## Exp-2 

## Comparative Analysis of Naïve Prompting versus Basic Prompting Across 

Name : Mukesh Kumar J.P 
Reg No:212222223002 
```
Various Test Scenarios 
1.	Introduction 
The effectiveness of AI models depends largely on how prompts are structured. A wellcrafted prompt can yield more relevant, accurate, and in-depth responses, while a vague or broad prompt may result in generic or incomplete answers. This study examines how different AI models respond to naïve prompts (broad and unstructured) versus basic prompts (clearer and more refined). The objective is to analyze the quality, accuracy, and depth of the responses across multiple scenarios and models. 
2.	Methodology 
This experiment compares responses from multiple AI language models by testing them with two types of prompts: 
•	Naïve prompts – Broad or unstructured questions that provide minimal guidance. 
•	Basic prompts – More specific and refined queries that provide clear context. 
Test Scenarios 
To ensure a well-rounded evaluation, the following categories were used: 
1.	General Knowledge: 
o	Naïve: "Tell me about black holes." o 	Basic: "Explain the formation and lifecycle of a black hole." 
2.	Creative Writing: 
o	Naïve: "Write a story." o Basic: "Write a short sci-fi story about AI gaining emotions." 
3.	Coding Assistance: 
o Naïve: "Help me with Python." o Basic: "How do I write a Python script to scrape web data?" 
4.	Problem-Solving & Advice: 
o	Naïve: "Give me advice on business." 
o	Basic: "What are the key steps to launching a startup in 2024?" 
5.	Opinion/Analysis: 
o 	Naïve: "What do you think about AI?" o 	Basic: "Analyze the impact of AI on the job market over the next decade." 
AI Models Evaluated 
We tested multiple models to compare performance: 
•	ChatGPT-4 
•	Claude-3 
•	Google Gemini 
•	LLaMA-3 
Each model was evaluated based on: 
✔	Depth & Detail – Completeness of response 
✔	Accuracy – Factual correctness 
✔	Clarity – Ease of understanding 
✔	Relevance – Alignment with the prompt 
✔	Conciseness vs. Verbosity – Balance of information 
✔	Creativity & Coherence – For creative writing prompts 
  
3. Results & Findings 
General Knowledge Prompt Responses 
Model 	Naïve Prompt Response 	Basic Prompt Response 
Provided a structured breakdown of 
ChatGPT- Gave a broad overview but lacked 
4 	details on formation/lifecycle. 	black hole formation, stages, and 
impact. 
Claude-3 Similar to ChatGPT but slightly 	More refined, included specific 
	more scientific in tone. 	astrophysics concepts. 
Gemini 	Detailed but contained minor 	More accurate with clear 
	factual inconsistencies. 	categorization. 
LLaMA-3 Simplistic and lacked depth. 	Improved structure but still less 
detailed than others. 
Key Insight: All models improved significantly with the refined prompt. LLaMA-3 had the weakest responses in depth, while ChatGPT-4 and Claude-3 provided the best structured answers. 
Creative Writing Prompt Responses 
Model 	Naïve Prompt Response 	Basic Prompt Response 
ChatGPT- A generic story with basic plot 	More engaging, creative, and cohesive 
4 	elements. 	narrative with strong themes. 
Claude-3 Similar to ChatGPT but leaned Highly refined storytelling with nuanced towards emotional depth. character development. 
Gemini 	Unique but sometimes incoherent Improved clarity, but less creative than in story progression. 	Claude. 
LLaMA-3 Basic storytelling with minimal 	Improved slightly, but still the weakest originality. 	in creativity. 
Key Insight: AI models produce much better creative writing when given specific direction. Claude-3 excelled in emotional storytelling, while ChatGPT-4 balanced structure and creativity well. 
Coding Assistance Prompt Responses 
Model 	Naïve Prompt Response 	Basic Prompt Response 
ChatGPT- Suggested Python basics but 
4 	lacked focus. 	Provided a complete script with explanations. 
Claude-3 Similar to ChatGPT but slightly less detailed. 	Gave a well-structured response with potential optimizations. 
Gemini 	Attempted a response but lacked full script structure. 	Improved with basic script but had minor errors. 
LLaMA-3 Generic Python tips, no actual code. 	Code snippet provided but lacked explanation. 
Key Insight: The refined prompt led to significantly better responses. ChatGPT-4 and Claude-3 delivered the best solutions, while LLaMA-3 struggled with technical depth. 
Business Advice Prompt Responses 
Model 	Naïve Prompt Response 	Basic Prompt Response 
ChatGPT-	Clear step-by-step guide with 
Generic, surface-level advice. 
4 	actionable insights. 
Model 	Naïve Prompt Response 	Basic Prompt Response 
Claude-3 	More strategic focus but lacked 	Well-organized and included risk structure. 	assessment. 
Gemini 	Motivational but vague. 	Improved structure but lacked depth. 
LLaMA-3 Cliché business tips. 	Better but still lacked depth. 
Key Insight: Business-related advice benefited greatly from refinement. ChatGPT-4 and Claude-3 provided the most actionable insights. 
Opinion/Analysis Prompt Responses 
Model 	Naïve Prompt Response 	Basic Prompt Response 
ChatGPT- General thoughts on AI without In-depth analysis with historical context 
4 	specifics. 	and future predictions. 
Claude-3 	More philosophical than 	Balanced approach with strong analytical. 	argumentation. 
Gemini Inconsistent, mix of general and Improved but lacked deep analysis. insightful points. 
LLaMA-3 Repetitive points on AI benefits 	Slightly better but still less structured. and risks. 
Key Insight: Opinion-based questions required specificity to get deeper responses. 
ChatGPT-4 and Claude-3 excelled, while Gemini and LLaMA-3 were less consistent. 
  
4. Conclusion & Takeaways 
✅ Refined prompts lead to significantly better responses across all AI models. Naïve prompts tend to yield generic and surface-level answers. 
✅ ChatGPT-4 and Claude-3 consistently delivered the best responses in most categories. They provided the most structured, accurate, and creative outputs. 
✅ Google Gemini showed inconsistency. It improved with refined prompts but had occasional factual inaccuracies or incoherent responses. 
✅ LLaMA-3 struggled with depth and accuracy. It improved slightly with refined prompts but lagged behind other models, especially in technical and analytical topics. 
✅ For creative writing, Claude-3 excelled. It produced more emotionally engaging and well-crafted stories. 
✅ For coding and structured explanations, ChatGPT-4 led the pack. It provided the best Python-related responses with well-structured explanations. 
 
```
